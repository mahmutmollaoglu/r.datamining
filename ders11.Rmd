---
title: "Veri Tipi Belirleme"
author: "mahmut mollaoglu"
date: '2022-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Çalışacağımız veri setinin içeri aktarılması için `read.csv` fonksiyonu kullanılır.

```{r}
dataset <- read.csv("https://bahadirfyildirim.com/media/documents/Fin500.csv")
```

Veriseti yüklendikten sonra veriseti içeriğini kontrol etmek üzere 'head()' ve 'tail()' fonksiyonları kullanılır.

```{r}
head(dataset,15) #Verisetinin ilk 15 satırını göster.
tail(dataset,15) #Verisetinin son 15 satırını göster.
```

Yüklenen verisetinin yapısını öğrenmek için 'str()' fonksiyonu kullanılır.

```{r}
str(dataset) #Verisetinin yapısını gösterir. Verisetine ait yapıların karakter mi yoksa sayısal mı olduğunu gösterir.
```

Yüklenen verisetinin özetini görmek için 'summary()' fonksiyonunu kullanırız.

```{r}
summary(dataset) #Verisetinin özetini gösterir. Verisetine ait değişkenlerin her birisine ait minimum, maksimum vb. istatistiki verilerini verir.
```

Yüklenen veri setindeki değişkenlerin bir kısmı yanlış hedeflenmiştir. Bu yüzden ilk olarak verinin yapısının değiştirlmesi adına farklı düzenlemeleri yapılmalıdır. Verilerin düzenlenmesi için ' <- as.factor()' komutu kullanılır.

```{r}
dataset$Industry <- as.factor(dataset$Industry) #Yüklenen verisetindeki değerlerin numerikten faktöre çevrilmesini sağlar.
```

Aynı işlem Inception ve State değişkenleri için de yapılır.

```{r}
dataset$Inception <- as.factor(dataset$Inception)
dataset$State <- as.factor(dataset$State) #Yüklenen verisetindeki değişkenlerin numerikten faktöre çevrilmesini sağlar.
```

Verisetindeki Expenses değişkeninde bulunan dollars ifadesi gereksiz görüldüğü için boşluğa dönüşütürülmüştür. Bunun için 'gsub' komutu kullanılmıştır.

```{r}
dataset$Expenses <- gsub(" Dollars", "", dataset$Expenses)
dataset$Expenses <- gsub(",", "", dataset$Expenses)
```

Verisetindeki growth değişkenine ait yüzde ifadelerin kaldırılması ve değişkenin faktörden numeriğe çevrilmesi için aşağıdaki fonksiyon kullanılır.

```{r}
dataset$Growth <- as.numeric(gsub("%", "", dataset$Growth))
```

R programlama dilinde herhangi bir ifadeye sahip olmayan $ ifadesinin bir anlam ifade etmediğini belirlemek adına başına \\ fonksiyonu kullanılır. 

```{r}
dataset$Revenue <- gsub("\\$", "", dataset$Revenue)
```

Yapılan düzenlemeler sonrasında verisetinin yapısını aşağıdaki komutla ortaya çıkartabiliriz.

```{r}
str(dataset)
```

Missing Value değerleriyle ilgilenileceğinden dolayı `library(Amelia)` paketini çağırılır.

```{r}
library(Amelia)
```

Verisetinin kayıp değerlerinin haritasını çıkartmak için 'missmap()' komutu uygulanır.

```{r}
missmap(dataset)
```

Verisetinin kaç satır ve kaç sütundan oluştuğunu görebilmek adına 'dim()' komutu uygulanır.

```{r}
dim(dataset)
```

Verisetimizde istediğimiz satır ve sütunlara erişebilmek için `dataset[]` komutu uygulanır.

```{r}
dataset[4,5] #4.satır 5.sütun
dataset[3, ] #3.satırın tamamı
dataset[ ,2] #2.sütunun tamamı
```

Verisetimizin uygun olmayan değer (NA) içerip içermediğini görebilmek için `complete.cases()` komutu uygulanır. Verisetimiz tam yani NA değer yoksa özette TRUE, NA değer varsa FALSE olarak geri dönüş vermektedir.

```{r}
complete.cases(dataset)
```

Verisetindeki NA değerlerinin yer aldığı satırı göstermek adına;

```{r}
dataset[!complete.cases(dataset), ] # ünlem işareti ! ile incomplete olanları getirilir.
```

Verisetimizde geliri 9 milyon olanları getirmek için aşağıdaki komut kullanılır.

```{r}
dataset$Revenue == 9746272
```

Verisetinde geliri 9 milyon olan SATIRI getirmek için aşağıdaki komut kullanılır.

```{r}
dataset[dataset$Revenue == 9746272, ]
```

True'ların yanında NA'lar duruyor. Bundan kaçınmak için `which()` komutunu kullanılır.. Bu komut uygulaması aşağıdaki gibidir;

```{r}
which(dataset$Revenue == 9746272)
```

Verisetimizde NA değeri olan satırları getirmek için aşağıdaki komut uygulanır.

```{r}
dataset[is.na(dataset$Expenses), ]
```

Verisetinde çalışan sayısı 45 olan satırı getirmek için aşağıdaki komut kullanılır.

```{r}
dataset[which(dataset$Employees == 45), ]
```

verisetimizi yedeklemek için `<-` komutu kullanılır.

```{r}
backup <- dataset
```

Verisetimizde complete olanlar ile tekrar veriseti oluşturmak adına aşağıdaki komutu kullanırız.

```{r}
dataset <- dataset[complete.cases(dataset), ]
```

Verisetindeki satır numaralarını görmek için `row.names()` komutu kullanılır.

```{r}
row.names(dataset)
```

Verisetimizde 1'den başlayarak tekrar satır numarası atamak için `1:nrow()` komutu kullanılır.

```{r}
dataset <- 1:nrow(dataset)
```

Kayıp verilerin değiştirilmesi için ilk olarak State sütununda yer alan NA değerleri çağrılır. Daha sonra da NA verileri değiştirilir.

```{r}
dataset[!complete.cases(dataset)]
dataset[is.na(dataset$State), ]
dataset[is.na(dataset$State) & dataset$City == "New York", ]
dataset[is.na(dataset$State) & dataset$City == "New York", "State"] <- "NY" 

dataset[is.na(dataset$State) & dataset$city == "San Francisco"]
dataset[is.na(dataset$State) & dataset$City == "San Francisco", "State"] <- "CA"
```

Verisetimizde NA değeri olan sütunları tespit ettikten sonra "Employees" sütununda yer alan NA değerleri yerine aynı sütundaki meadian değerini atadık. "Employees" sütunu ile "Industry" sütununda "Retail" olanları atanır. Daha sonra "Industry" sütununda "Financial Services" olanları atanır.

```{r}
summary(dataset$Employees)
dataset[is.na(dataset$Employees), ]

median(dataset[, "Employees"]) # NA baskın olduğu için sonuç NA çıktı bundan kurultmak için aşağıda verilen komut girilmiştir. 
median(dataset[, "Employees"], na.rm = T) #NA ya median değerini ata NA'yı dikkate alma
med_emp_ret <- median(dataset[dataset$Industry == "Retail", "Employees"], na.rm = T)

dataset[is.na(dataset$Employees) & dataset$Industry == "Retail", ]
dataset[is.na(dataset$Employees) & dataset$Industry == "Retail", "Employees"] <- med_emp_ret

med_emp_finservices <- median(dataset[dataset$Industry == "Financial Services", "Employees"], na.rm = T) 
dataset[is.na(dataset$Employees) & dataset$Industry == "Financial Services", "Employees"] <- med_emp_finservices
```

Expenses kolonunda NA olan değerler "Revenue"'den "Profit"i çıkartarak elde ederiz.

```{r}
summary(dataset)
head(dataset,25)

dataset[is.na(dataset$Expenses), ]
dataset[is.na(dataset$Expenses) & !is.na(dataset$Revenue), ] #Revenue boş olmaması lazım
dataset[is.na(dataset$Expenses) & !is.na(dataset$Revenue), "Expenses" ] #expenses kolonunu verecek

dataset[is.na(dataset$Expenses) & !is.na(dataset$Revenue), "Expenses" ] <- dataset[is.na(dataset$Expenses) & !is.na(dataset$Revenue), "Revenue" ] - dataset[is.na(dataset$Expenses) & !is.na(dataset$Revenue), "Profit" ] 
```

"Revenue" kolonunda NA olan değerleri kolonun ortalamasını NA olan değerlere atayarak elde ederiz.

```{r}
summary(dataset)
head(dataset,25)

dataset[is.na(dataset$Revenue), ]
dataset[is.na(dataset$Revenue) & dataset$Industry == "Construction", ] #Revenue NA olan Industry'ler Construction

mean(dataset[, "Revenue"])
mean(dataset[, "Revenue"], na.rm = T)
mea_rev <- mean(dataset[, "Revenue"], na.rm = T)

dataset[is.na(dataset$Revenue) & dataset$Industry == "Construction", "Revenue"] <- mea_rev
``` 


Çalışılan veri setinde daha anlamlı bir model oluşturmak ve onu değerlendirmek için 'caret' paketi yüklenir. Yüklenmesi için aşağıdaki komut yüklenir.

```{r}
library(caret)
```

```{r}
summary(iris[,1:4])
```

Iris veri seti içerisindeki 1 ve 4. sütun aralığını verir.

Sonrasında veri pre-process (ölçekleme, merkezleme) sürecine alınır. Bu süreç aşağıdaki gibi gösterilebilir.

```{r}
preprocessParams <- preProcess(iris[,1:4], method=c("scale"))
```

Yukarıda verilen 'scale' fonksiyonu her bir gözlemi o serinin standart sapmasına oranlar.

Parametrelerin genel özetinin kullanıldığı komut aşağıda verilmiştir.

```{r}
print(preprocessParams)
```

Birinci ve dördüncü sütun aralığını ölçekliye çevirmek için aşağıdaki komut kullanılır.

```{r}
scaled <- predict(preprocessParams, iris[,1:4])
```

Sonrasında dönüştürülen veri setinin özeti alınır.

```{r}
summary(scaled)
```

Aşağıda belirtilecek olan 'center' fonksiyonu ise her bir değişkenin ortalamalarından çıkartılmış hâlini bize verir.

```{r}
preProcessparams <- preProcess(iris[, 1:4] ,method=c("center","scale"))
```

Bu parametlerelerin normalizasyon işlemi ise aşağıdaki gibi gerçekleştirilebilir.

```{r}
normalized <- predict(preprocessParams, iris[, 1:4])
```

Veri setinin pre-process parametlerelerinin hesaplanması içinse aşağıdaki fonksiyon kullanılır.

```{r}
preprocessParams <- preProcess(iris[, 1:4], method=c("YeoJohnson")) print(preprocessParams)
yeojohnson <- predict(preProcessparams, iris[, 1:4])
```

Son olarak düzenlenen veri setinin özeti alınır.
```{r}
summary(yeojohnson)
```

Bu haftada yeniden örnekleme yöntemlerine bakılacaktır. İlk olarak veri setimiz geri çağrılır. Bunun için aşağıdaki işlemler gerçekleştirilir.

```{r}
data(iris)
summary(dataset)
dataset <- iris
```

Bootstrap ile 100 örneklem oluşturulur. Bu örneklemin oluşturulmasındaki yapılması gereken işlem aşağıdaki gibidir.

```{r}
train_control <- trainControl(method="boot", number=100)
```

Verisetinin %80'ini ayıran komut için aşağıdaki işlemi takip ederiz;

```{r}
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
data_train <- iris[ trainIndex, ]
data_test <- iris[ -trainIndex, ]
```

Leave One Out Cross Validation yöntemiyle örnekleme yapmak için ilk caret package'ı aktif edilir. Sonrasında örneklem yönteminin komutu yazılır. Bu adımlar aşağıdaki gibi gösterilebilir.

```{r}
Library(caret)
data(iris)
train_control <-trainControl(method="LOOCV")
```

K-Fold Cross Validation için aşağıdaki adımlar yürütülür.

```{r}
data(iris)
train_control <- trainControl(method="cv", number=10)
```

Son olarak Repeated Cross Validations yöntemini uygulamak içinse aşağıdaki adımlar uygulanır.

```{r}
data(iris)
train_control <- trainControl (method="repeatedcv", number=10, repeats=3)
```

Bu haftada veri setinin regresyon modeline uygun olup olmadığı test edilmiştir.
Verisetinin regresyon modeli uygun olup olmadığını ile eğrisel mi yoksa doğrusal mı olduğu `scatter.smooth()` kodu ile gösterilmiştir. Kodun yazılımı aşağıdaki gibi gösterilebilir.

```{r}
scatter.smooth(x=cars$speed, y=cars$dist, main="Saçılma Diyagramı")
```

Korelasyon üretmek için `cor()` kodu kullanılmıştır.

```{r}
cor(cars$speed, cars$dist)
```

Regresyon modelini oluşturmak için `lm()` fonksiyonu kullanılmıştır ve bu 'GenelModel'e atanmıştır. Bunun için yapılan kod fonksiyonu aşağıdaki gibi gösterilebilir.

```{r}
GenelModel <- lm(dist ~ speed, data=cars) # lm(bağımlı ve bağımsız değişken) (lm, bağımlı değişken, bağımsız değişken , dataset)
```

GenelModel içerisindeki tahmini 'distance'ı almak için `print()` ve daha sonra GenelModel'in özetini görmek için `summary()` komutunu kullanılmıştır.

```{r}
print(GenelModel)
summary(GenelModel)
```

Bilgi kriterlerinin bulunabilmesi için `AIC()` ve `BIC()` komutları kullanılmıştır.

```{r}
AIC(GenelModel) 
BIC(GenelModel)
```

Veri setinden rastgele değerler oluşturabilmek için `set.seed()` fonksiyonu kullanılmıştır. Aşağıda belirtilen 100 sayısı belirli bir veri seti kümesini ifade eder. Başka bir bilgisayarda yine aynı sayıyla aynı veri kümesi elde edilir. Ancak verilen sayı değişirse rassal değerlerde de değişkenlik oluşur.

```{r}
set.seed(100)
```

Satır index değerleri `sample()` fonksiyonu ile oluşturulmuş ve değerler 'trainingRowIndex'e atanmıştır.

```{r}
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars)) # 1'den n row'a kadar verinin yüzde 80'ini rassal olarak seçtik.
```

cars verisetindeki daha önce oluşturduğumuz trainingRowIndex satırı ve tüm sütunu 'trainingData'ya atanmıştır.

```{r}
trainingData <- cars[trainingRowIndex, ] #satırlarını al ve tüm sütunlarını al
```

'testData'ya 'trainingRowIndex'in tersi atanmıştır.

```{r}
testData <- cars[-trainingRowIndex, ]
```

Tekrar Regresyon modeli oluşturmak için `lm()` fonksiyonu kullanılmış ve bu 'lmMod'a atanmıştır. Daha sonra testData'sının tahmini değerlerini `predict()` komutu ile alınmış ve `print()` `summary()` ile `ÀIC()` komutları ile verisinin içeriğine bakılmıştır. Bu işlemler için kullanılan fonksiyonlar aşağıdaki gibi verilmiştir.

```{r}
lmMod <- lm(dist ~ speed, data=trainingData)
distPred <- predict(lmMod, testData) 
print(distPred)
summary(lmMod)
AIC(lmMod)
```

Gözlemlenen ve tahmin edilen verileri birleştiren komuta (bind) adı verilir. Yani iki vektörü birleştirir (bind) ve yeni bir vektör oluşturur. Bunun için `data.frame()` fonksiyonundan yararlanılmıştır. Ve yeni bulunan vektör 'gercek_tahmin'e atanmıştır.

```{r}
gercek_tahmin <- data.frame(cbind(gercek=testData$dist, tahmin=disttahmin)) 
```

mape (mean absolute percentage error) yani gerçek değer ie tahmini değer arasındaki farkın mutlak değerini alarak gerçek değere oranlanır. Diğer bir deyişle hata payını alarak gerçek değere oranlanır. Ve `print()` komutu ile çıktısını alınır.

```{r}
mape <- mean(abs(actuals_preds$tahmin - actuals_preds$gercek)/actuals_preds$gercek)
print(mape)
```

